{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa70776",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re, os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8553caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    bos_token = '<s>'\n",
    "    eos_token = '</s>'\n",
    "    usr_token = '<usr>'\n",
    "    pad_token = '<pad>'\n",
    "    sys_token = '<sys>'\n",
    "    unk_token = '<unk>'\n",
    "    mask_token = '<mask>'\n",
    "    max_length = 2 ** 8\n",
    "    max_turns = 3\n",
    "    pretrained_model_name = \"skt/kogpt2-base-v2\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = 'model.pt'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e768771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622362e722704f81821e369b707bff59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cbbf690be34c93853cc90998c73ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1354544ecaca476c93903a2f6c3e3bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.pretrained_model_name,\n",
    "            bos_token=Config.bos_token, eos_token=Config.eos_token, unk_token=Config.unk_token,\n",
    "            pad_token=Config.pad_token, mask_token=Config.mask_token, model_max_length=Config.max_length)\n",
    "model = AutoModelForCausalLM.from_pretrained(Config.pretrained_model_name).to(Config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(Config.model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_ids = [tokenizer.encode(token) for token in [Config.unk_token,\n",
    "                'ㅋ', 'ㅎ', 'ㅋㅋ', 'ㅎㅎ', 'ㅜ', 'ㅠ', 'ㅜㅜ', 'ㅠㅠ']]\n",
    "history_limit = [Config.bos_token]\n",
    "usr_token_id = tokenizer.convert_tokens_to_ids(Config.usr_token)\n",
    "sys_token_id = tokenizer.convert_tokens_to_ids(Config.sys_token)\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"user > \")\n",
    "\n",
    "    if user_message == \"끝\": break\n",
    "        \n",
    "    if user_message == \"초기화\":\n",
    "        history_limit = ['<s>']\n",
    "        continue\n",
    "        \n",
    "    if len(history_limit) == Config.max_turns * 2 + 1:\n",
    "        history_limit = history_limit[: 1] + history_limit[3: ]\n",
    "        \n",
    "    user_message_pull = Config.usr_token + user_message + Config.sys_token\n",
    "\n",
    "    history_limit.append(user_message_pull)\n",
    "\n",
    "    message_ids = tokenizer.encode(''.join(history_limit),\n",
    "                                   return_tensors=\"pt\").to(Config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reply_ids = model.generate(\n",
    "            message_ids,\n",
    "            max_new_tokens=30,\n",
    "            early_stopping=True,\n",
    "            num_beams=3, # ★\n",
    "            temperature=0.95, # ★\n",
    "            top_p=0.92, # ★\n",
    "            repetition_penalty=1.1, # ★\n",
    "            no_repeat_ngram_size=3,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            forced_eos_token_id=usr_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=[usr_token_id, sys_token_id]\n",
    "        )\n",
    "        \n",
    "    decoded_ids = reply_ids[0, message_ids.shape[-1]: -1]\n",
    "\n",
    "    decoded_message = tokenizer.decode(decoded_ids)\n",
    "    \n",
    "    history_limit.append(decoded_message)\n",
    "\n",
    "    print(\"system > \", decoded_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2bc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_words_ids = [tokenizer.encode(token) for token in [Config.unk_token,\n",
    "                'ㅋ', 'ㅎ', 'ㅋㅋ', 'ㅎㅎ', 'ㅜ', 'ㅠ', 'ㅜㅜ', 'ㅠㅠ']]\n",
    "history_limit = [Config.bos_token]\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"user > \")\n",
    "\n",
    "    if user_message == \"끝\": break\n",
    "        \n",
    "    if user_message == \"초기화\":\n",
    "        history_limit = ['<s>']\n",
    "        continue\n",
    "        \n",
    "    if len(history_limit) == Config.max_turns * 2 + 1:\n",
    "        history_limit = history_limit[: 1] + history_limit[3: ]\n",
    "        \n",
    "    user_message_pull = Config.usr_token + user_message + Config.sys_token\n",
    "\n",
    "    history_limit.append(user_message_pull)\n",
    "\n",
    "    message_ids = tokenizer.encode(''.join(history_limit),\n",
    "                                   return_tensors=\"pt\").to(Config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reply_ids = model.generate(\n",
    "            message_ids,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            temperature=0.95,\n",
    "            top_p=0.92,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=3,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            forced_eos_token_id=usr_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=[usr_token_id, sys_token_id]\n",
    "        )\n",
    "        \n",
    "    system_ids = reply_ids[0, message_ids.shape[-1]: -1]\n",
    "\n",
    "    decoded_message = tokenizer.decode(decoded_ids)\n",
    "    \n",
    "    history_limit.append(decoded_message)\n",
    "\n",
    "    print(\"system > \", decoded_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22517a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
